# **Recent Advances in Spatially-Aware Text-Image Retrieval and Multimodal Grounding (2024-2025): A Literature Review and Comparative Analysis**

## **1\. Introduction**

This report reviews and analyzes recent advancements (specifically within the 2024-2025 timeframe) in the field of spatially-aware textual image search. The objective is to contextualize a specific proposed system – which utilizes text n-grams indexed with normalized bounding box coordinates and employs an explicit geometric scoring function based on Intersection over Union (IoU) and proximity – within the rapidly evolving research landscape. The analysis focuses on identifying dominant trends, novel techniques, related task advancements, and emerging evaluation methodologies relevant to querying images based on text content found in specific spatial locations.

A primary observation from the 2024-2025 literature is the pronounced shift towards leveraging Multimodal Large Language Models (MLLMs) as the predominant approach for tackling tasks involving spatial grounding – the linking of textual descriptions to specific regions or objects within visual data.4 These models often integrate spatial reasoning implicitly through learned representations, contrasting sharply with methods relying on explicit geometric indexing and calculation. This review will delve into these MLLM-based approaches, examine progress in related areas such as Referring Expression Comprehension (REC) and Document Visual Question Answering (DocVQA), discuss new evaluation paradigms focused on groundedness, synthesize these findings to position the aforementioned n-gram/geometric system, and conclude with recommendations for incorporating these recent developments.

## **2\. The Ascendancy of MLLMs in Spatial Grounding (2024-2025)**

The period between 2024 and 2025 has witnessed a significant paradigm shift in addressing spatial relationships between text and image content. While earlier methods, including the system under consideration, often focused on explicit geometric representations (like bounding boxes) and rule-based scoring (like IoU/proximity), the current trend heavily favors end-to-end MLLMs.4 These models aim to achieve a more holistic understanding by integrating visual perception, language comprehension, and spatial reasoning capabilities within a unified architecture, often trained on vast datasets.6

### **2.1 Key Models and Architectures**

Several notable MLLMs demonstrating advanced grounding capabilities have emerged in 2024:

* **KOSMOS-2 (Microsoft Research, ICLR 2024):** Building upon its predecessor KOSMOS-1, KOSMOS-2 explicitly introduces multimodal grounding capabilities.7 A key enabler is the creation of GRIT, a web-scale dataset of grounded image-text pairs derived from sources like LAION-2B and COYO-700M.7 The model architecture converts continuous bounding box coordinates (e.g., \[top, left, bottom, right\]) associated with text spans (noun phrases, referring expressions) into sequences of discrete location tokens. These location tokens are effectively treated as part of the model's vocabulary and are integrated into the input sequence, often appended to the corresponding text span using a special "hyperlink" format. The MLLM is then trained on the combined multimodal corpora (including GRIT) to learn the mapping between image regions, their corresponding location tokens, and the associated text descriptions.7 KOSMOS-2's effectiveness is demonstrated through evaluations on multimodal grounding tasks, including REC and phrase grounding, aiming for a more flexible human-computer interface for vision-language tasks.7  
* **Groma (FoundationVision, ECCV 2024):** Groma is presented as a grounded multimodal assistant specifically designed for "exceptional region understanding and visual grounding capabilities".1 It employs a technique termed "localized visual tokenization," suggesting a mechanism to represent spatial information finely within the model's processing pipeline. Groma achieves state-of-the-art performance among MLLMs on standard REC benchmarks like RefCOCO, RefCOCO+, and RefCOCOg, underscoring its strong localization abilities.1 Its development involves a multi-stage training strategy: initial detection pretraining (using models like DINOv2 and datasets like COCO, Objects365), followed by alignment pretraining (using image captions, grounded captions from Flickr30k Entities, region captions from Visual Genome, and REC data), and finally, instruction finetuning (using instruction-following datasets like LLaVA Instruct and a custom 30k visually grounded conversation dataset, Groma Instruct).1 This extensive training regimen highlights the complexity and data requirements for building such high-performing grounded MLLMs.

Beyond these specific examples, the broader trend involves applying MLLMs to achieve universal multimodal retrieval 4, enhance contextual understanding 4, improve multimodal query representations 4, and delve into the interpretability of their internal representations concerning multimodal concepts and grounding.5

### **2.2 Spatial Representation and Linking Mechanisms**

MLLMs employ several strategies to represent and reason about spatial information, moving away from explicit geometric calculations at query time:

* **Location Tokens:** As exemplified by KOSMOS-2 7 and likely used in similar forms by models like Groma 1, continuous coordinates are discretized into a finite set of special tokens. An image's width and height might be divided into segments (e.g., P segments each), allowing any coordinate pair or bounding box to be represented as a short sequence of these tokens.7 These tokens are then processed by the language model component alongside text tokens. While this facilitates integration into the LLM framework, the discretization step inherently involves a quantization of spatial information, potentially sacrificing fine-grained precision compared to continuous coordinate systems.  
* **Attention Mechanisms:** The Transformer architecture, fundamental to most MLLMs, relies heavily on attention mechanisms. Cross-modal attention layers allow the model to learn correlations between elements of the text input (e.g., words in a referring expression) and specific regions or features in the visual input. This learned attention implicitly captures spatial relationships, enabling the model to "focus" on the relevant image area when processing a spatially-related query, without needing explicit geometric checks during inference. Earlier work like ESA explored explicit spatial dimensional attention 3, and MLLMs build upon these concepts within more complex architectures.  
* **Learned Joint Embeddings:** MLLMs are trained to map inputs from different modalities (vision, language) into a shared or aligned embedding space.5 Effective grounding relies on ensuring that representations of image regions are embedded closely to the representations of their corresponding textual descriptions, potentially including location information encoded implicitly or explicitly (e.g., via location tokens). This alignment is typically achieved through contrastive learning or other objectives on large paired datasets.6

### **2.3 Comparison with Explicit Geometric Indexing**

Contrasting the dominant MLLM approach with the n-gram/geometric indexing method reveals fundamental differences:

* **Spatial Reasoning:** The draft paper's system uses *explicit* geometric calculations (IoU, proximity) on indexed, normalized bounding boxes at query time. MLLMs perform *learned*, often *implicit*, spatial reasoning embedded within their neural network operations, guided by patterns discovered during training on large grounded datasets.  
* **Semantic Understanding:** MLLMs possess inherent semantic understanding capabilities derived from their underlying language models. They can handle synonyms, paraphrasing, and contextual nuances in queries (e.g., understanding "large vehicle" might relate to a "truck" in the image). The draft's system, based on exact n-gram matching, lacks this semantic flexibility.  
* **Query Flexibility:** MLLMs can process complex natural language queries describing objects and their spatial relationships (e.g., "the cat sitting to the left of the sofa"). KOSMOS-2, for instance, aims for a flexible interface supporting such interactions.7 The draft system is limited to matching specific text n-grams within an optional rectangular query region.  
* **Interpretability and Control:** The draft system offers higher interpretability; its scores are directly tied to understandable geometric measures (IoU, proximity), and their relative importance can be explicitly controlled via weights. MLLM reasoning is often opaque ("black box"), making it harder to understand *why* a particular region was grounded or how to fine-tune spatial preferences directly.  
* **Data Requirements:** MLLMs like KOSMOS-2 and Groma require massive, diverse datasets for pretraining and finetuning, including specialized grounded datasets like GRIT or Flickr30k Entities.7 The draft system's indexing phase depends primarily on the availability and quality of OCR output (or pre-computed metadata in the synthetic case), potentially requiring less *grounded* training data, although building a large image index still requires significant data.

The widespread application of similar MLLM-based grounding techniques across diverse fields points towards a fundamental, shared need for integrating spatial understanding with language processing in AI. We see these models applied not only in core vision-language tasks like REC 7 but also in specialized domains like autonomous driving for interpreting commands within visual scenes 9, in document analysis for locating specific information fields 10, and even in conversational AI for resolving references across modalities.12 This convergence suggests that the underlying challenge of grounding language in spatial visual context is pervasive, and MLLMs are increasingly viewed as a potentially unifying technology. Consequently, advancements or architectural innovations developed for grounding in one domain may rapidly propagate to others, accelerating overall progress in spatially-aware AI.

Furthermore, many recent MLLM approaches conceptualize grounding as a *generative* task rather than purely a retrieval one. Models like KOSMOS-2 7 and Groma 1 can generate textual descriptions that include grounded references (e.g., outputting text interleaved with location tokens pointing to image regions) or act as interactive assistants capable of discussing localized content. This represents a shift from systems that merely *find* pre-existing items based on spatial criteria (like the draft's retrieval focus) towards models that can actively *produce* grounded language. While offering richer interaction possibilities, this generative capability also introduces challenges common to LLMs, such as the potential for hallucination – generating plausible but factually incorrect spatial references – an issue highlighted in the context of DocVQA.10

The successful development and training of these powerful MLLMs, however, appear heavily reliant on the availability of large-scale, high-quality datasets where text spans are accurately linked to their corresponding image regions (i.e., grounded data). The significant effort invested in creating the GRIT dataset for KOSMOS-2 7 and the use of multiple existing and custom grounded datasets for Groma 1 underscore this dependency. This suggests that the availability and cost of creating such grounded datasets 14 may act as a significant bottleneck, potentially gating future progress in the field. Innovations in automated or semi-automated methods for generating grounded annotations could therefore be crucial for scaling these approaches further.

## **3\. Advances in Related Grounding Tasks (REC, Document VQA, etc.)**

Progress in spatially-aware retrieval is often intertwined with advancements in related tasks that heavily rely on visual grounding. Developments in these areas serve as both drivers and benchmarks for grounding capabilities.

### **3.1 Referring Expression Comprehension/Generation (REC/REG)**

REC (locating an image region described by a natural language phrase) and REG (generating a phrase to uniquely identify a region) remain core tasks for evaluating fine-grained spatial grounding. MLLMs like KOSMOS-2 7 and Groma 1 are explicitly benchmarked on standard REC datasets (RefCOCO, RefCOCO+, RefCOCOg), with models like Groma claiming state-of-the-art performance among MLLMs.1 This demonstrates their ability to map nuanced textual descriptions to precise image locations. The use of REC/REG tasks extends beyond specific models, often included in broader multimodal evaluation suites.15 Additionally, research explores improving grounding in existing vision-language models *without* retraining, for example, using contrastive region guidance techniques.16

The consistent use of REC/REG as primary benchmarks for evaluating the grounding capabilities of general-purpose MLLMs 7 indicates that the research community considers strong performance on these tasks as indicative of fundamental visual grounding competence. This implies a likely transfer of methodologies; techniques, architectures, and training strategies proving successful for the specific challenge of REC are probably being integrated into the design of broader MLLM systems that require spatial understanding, thereby facilitating cross-pollination of ideas from specialized tasks to general models.

### **3.2 Document Visual Question Answering (DocVQA) and Grounding**

In the domain of DocVQA, which involves answering questions about visually rich documents (like forms, receipts, or reports), there is a growing emphasis on the *groundedness* of answers.10 It is often insufficient for a model to simply generate the correct text string; for practical applications, it is crucial to ensure the answer is derived from the document content and, ideally, can be localized to a specific region (text span or visual element) within the document.10 This need directly resonates with the motivation behind the draft paper, particularly the example of extracting specific fields like "Total Amount" from invoices based on their typical locations.

Researchers have pointed out that traditional VQA evaluation metrics (e.g., Exact Match, F1 score on the answer string) are inadequate as they fail to penalize ungrounded answers or hallucinations.10 A model might generate the correct answer text by chance or based on parametric knowledge, without actually "seeing" it in the document at the correct location. The challenge of reliably localizing or grounding the outputs of generative models within the source document remains a significant hurdle for deploying these systems in enterprise settings where provenance and verifiability are critical.10

The strong focus on reliable spatial localization within the recent DocVQA literature 10 serves as external validation for the core problem addressed by the draft paper. It highlights that even amidst the rise of powerful, general-purpose MLLMs, the specific challenge of accurately and verifiably linking extracted text to its precise location within structured or semi-structured documents (like the invoice example) remains a pertinent and unsolved problem. This suggests that approaches offering explicit geometric control and localization guarantees, like the draft's method, might still hold significant value in application areas where precision and verifiability are paramount, potentially complementing MLLMs where issues like hallucination or imprecise localization persist.

### **3.3 Other Relevant Domains**

Spatial grounding is also a critical component in other emerging application areas:

* **Autonomous Driving:** MLLMs are being explored to interpret linguistic commands within the visual context of driving scenes, requiring accurate grounding of concepts like "the red car ahead" or "the traffic light on the right" to elements perceived by the vehicle's sensors.9 Challenges include achieving real-time performance and maintaining robustness under adverse conditions (e.g., poor weather, complex urban environments).9  
* **Multimodal Co-reference Resolution:** In conversational AI, particularly with visual elements (e.g., shopping assistants displaying products), resolving references like "that one" or "it" requires linking the pronoun not just to prior text but potentially to specific objects or regions in an associated image across multiple dialogue turns.12  
* **Instruction Following:** Embodied agents or multimodal assistants need to ground instructions (e.g., "pick up the blue cup") in their perception of the environment, linking textual commands to visual objects and spatial locations.13

## **4\. Emerging Evaluation Techniques for Grounding**

The limitations of traditional text-based metrics in evaluating spatially-aware systems have spurred the development of new evaluation paradigms focused explicitly on grounding.

### **4.1 Limitations of Traditional Metrics**

Metrics like Exact Match (EM), F1-score, BLEU, or ROUGE, commonly used in NLP and VQA, primarily assess the surface-level similarity between generated text and ground-truth text. They are ill-suited for tasks where spatial accuracy is crucial because they do not consider *where* the information is located or *if* the generated answer corresponds to the correct visual entity.10 A system could produce a textually perfect answer that refers to the wrong object or location, yet still score highly on these metrics.

### **4.2 Focus on Groundedness**

Recent evaluation efforts emphasize "groundedness" – verifying that a model's output is not only textually relevant but also correctly linked to the appropriate evidence within the input modality (e.g., a specific region in an image or document).10 This addresses the critical issue of hallucinations and ensures that models are performing genuine multimodal reasoning rather than relying solely on parametric knowledge or statistical correlations.

### **4.3 SMuDGE Framework**

A notable development is the Semantics and Multimodal Document Grounded Evaluation (SMuDGE) framework, proposed specifically for DocVQA but with principles applicable more broadly.10 SMuDGE introduces a multi-component scoring approach:

1. **Semantic Alignment Score:** This component first checks if the generated answer's semantic type aligns with the expected type (e.g., if a number was expected, is the output a number?). This prevents simple textual similarity scores from rewarding outputs that are fundamentally incorrect in type.  
2. **Multimodal Grounding Score:** This core component assesses whether the text generated by the model can be found within the input document. If it can, it further measures the spatial relationship (e.g., overlap or proximity) between the location of the model's generated answer and the location of the ground-truth answer within the document. This directly evaluates the spatial accuracy of the prediction.

SMuDGE is designed to be configurable, allowing users to weight the importance of semantic alignment versus multimodal grounding based on the specific requirements of their downstream application.10 Validation using human judgments reportedly shows that SMuDGE scores align better with human preferences regarding answer quality compared to traditional metrics, and its application can potentially alter the ranking of models on existing leaderboards.10

The emergence of specialized grounding metrics like SMuDGE 10 signals that evaluation methodologies are actively evolving to keep pace with the advancing capabilities—and associated failure modes, such as spatial inaccuracies or hallucinations—of modern MLLMs. The fact that these metrics are being developed *in response* to the limitations observed when applying older metrics to newer, more complex models suggests a reactive dynamic. As models become more powerful and capable of generating nuanced, spatially-aware outputs, the need for evaluation tools that can rigorously assess the correctness of this spatial grounding becomes increasingly apparent. This implies that current benchmark leaderboards relying solely on traditional metrics might not fully capture the true spatial reasoning abilities or reliability of the systems being compared.

### **4.4 Relevance to the Draft Paper's Evaluation**

The principles behind SMuDGE are highly relevant for evaluating the draft paper's system, especially when transitioning to real-world data beyond the synthetic setup. While MAP and P@k assess the ability to rank the correct *image* highly, they do not measure *how well* the spatial constraint was satisfied *within* that image. Incorporating a spatial grounding score, inspired by SMuDGE's multimodal grounding component, could provide a much finer-grained evaluation. For instance, one could measure the IoU or proximity between the query region and the actual n-gram bounding box within the top-ranked relevant images.

This suggests a potential direction for more comprehensive evaluation in the future, combining traditional information retrieval metrics with grounding-specific assessments. For a task like the one addressed by the draft paper (retrieving images based on spatially constrained text), a hybrid evaluation could first use MAP/P@k to measure the system's effectiveness at retrieving the correct image documents within the top results. Subsequently, a grounding metric, akin to SMuDGE's spatial component 10, could be applied to these top-ranked images to specifically quantify how accurately the target text was localized relative to the spatial query constraints within those retrieved images. Such a hybrid approach would offer a more holistic picture, assessing both the system's ability to find the right content and its precision in adhering to the spatial requirements of the query.

## **5\. Synthesis: Positioning the Draft Paper in the Current Landscape (2024-2025)**

Synthesizing the findings from the 2024-2025 literature reveals a landscape increasingly dominated by MLLMs that leverage large-scale datasets and learned representations for spatial grounding. Progress in related tasks like REC and DocVQA continually pushes the boundaries of grounding capabilities and highlights the practical importance of spatial accuracy and verifiability. Concurrently, evaluation methodologies are evolving to measure groundedness more directly, moving beyond simple textual similarity.

To clarify the position of the draft paper's explicit geometric indexing approach relative to these recent trends, the following table provides a comparative overview:

**Table 1: Comparison of Recent (2024-2025) Spatial Grounding Approaches**

| Feature | Explicit Geometric Indexing (Draft Paper) | MLLM Grounding (e.g., KOSMOS-2 , Groma ) |
| :---- | :---- | :---- |
| **Approach Category** | Explicit Geometric Indexing & Scoring | Learned End-to-End Multimodal Modeling |
| **Query Input** | Text N-grams \+ Optional Rectangular Region | Natural Language (potentially complex spatial relations), Optional Pointing/Regions |
| **Spatial Representation** | Normalized Bounding Boxes | Learned Location Tokens, Attention Maps, Joint Embeddings |
| **Spatial Handling** | Explicit IoU & Proximity Calculation at Query Time | Implicitly Learned via Attention/Embeddings |
| **Indexing/Core Mechanism** | Inverted Index (N-gram \-\> List) | End-to-End MLLM (integrating Vision Encoder & LLM) |
| **Semantic Understanding** | None / Limited (Exact N-gram Match) | High (Leverages LLM capabilities) |
| **Key Strengths** | Interpretability, Direct Geometric Control, Potential Precision/Efficiency for Exact Spatial Match | Semantic Flexibility, Complex Query Handling, Generalization Potential, State-of-the-Art on Grounding Benchmarks |
| **Key Weaknesses** | Lack of Semantics, Limited Query Language, Exact Match Dependency, Potential Scalability Issues (In-Memory Index) | "Black Box" Reasoning, High Data/Compute Requirements, Potential for Hallucination, Discretization Limits (Location Tokens) |
| **Evaluation Focus** | MAP/P@k on Image Retrieval (Synthetic Data) | REC Accuracy, Grounded VQA Metrics (e.g., SMuDGE principles 10), Natural Language Evaluation |

Based on this comparison, the draft paper's system occupies a distinct niche. Its primary focus is on the *precise geometric localization of exact text n-grams* using an interpretable and explicitly controllable scoring mechanism. Its strengths lie in this precision for specific types of queries, the transparency of its scoring (directly tied to IoU and proximity), and the direct control afforded by tunable weights. The evaluation on synthetic data effectively isolates and validates this geometric localization capability. Furthermore, the demonstrated need for reliable localization in fields like DocVQA 10 reinforces the potential value of such an explicit approach, particularly where verifiability is crucial.

However, when viewed against the backdrop of 2024-2025 advancements, its limitations become more apparent. The lack of semantic understanding is a major drawback compared to MLLMs that can effortlessly handle synonyms, context, and complex linguistic descriptions.6 The reliance on exact n-gram matching and the restriction to simple rectangular queries further limit its applicability compared to the flexible natural language interfaces offered by MLLMs.7 While the in-memory inverted index might be efficient for moderate datasets, its scalability to web-scale collections could be a concern compared to potentially more complex but distributed MLLM inference frameworks. Dependence on the quality of upstream OCR remains a critical factor, although this challenge also affects MLLMs unless they incorporate end-to-end text spotting.

Despite these limitations, the draft's approach might serve a complementary role. Its potential efficiency and precision for exact-match geometric queries could make it suitable as a fast candidate filtering mechanism in a larger hybrid system. For instance, it could rapidly identify images containing specific text within a coarse region, with a more sophisticated (and computationally heavier) MLLM subsequently employed to re-rank these candidates based on finer semantic understanding or more complex spatial reasoning. Positioning the work as a strong, interpretable baseline or a potential component for hybrid architectures could be a constructive framing.

## **6\. Recommendations for Paper Update**

Based on the review of recent literature (2024-2025), the following updates to the draft paper, particularly Sections 2 ("Prior Work") and 6 ("Limitations" and "Future Work"), are recommended:

### **6.1 Updating "Prior Work" (Section 2\)**

* **Integrate MLLM Grounding:** Introduce a dedicated discussion on the significant trend of using MLLMs for spatial grounding. Cite and briefly describe key 2024 examples like KOSMOS-2 7 and Groma.1 Explain their general methodology (e.g., learned representations, location tokens, large-scale training) and explicitly contrast their learned, semantic-aware approach with the draft's explicit geometric indexing and scoring.  
* **Incorporate Related Task Insights:** Briefly mention recent developments in closely related areas that underscore the importance of grounding. Reference work showing performance gains in REC (potentially mentioning training-free methods like 16) and the critical focus on answer groundedness and localization in DocVQA.10 This helps contextualize the problem's broader relevance.  
* **Refine Comparison Table (Table 1 in Draft):** Consider revising or replacing the existing Table 1 in the draft paper. The new table should accurately reflect the current landscape, possibly drawing from Table 1 presented in Section 5 of this report. Ensure the comparison focuses on the most pertinent differences between the draft's approach and the dominant MLLM-based methods of 2024-2025.

### **6.2 Updating "Limitations" and "Future Work" (Section 6\)**

* **Acknowledge MLLM Context:** Explicitly frame the limitations regarding lack of semantic understanding and restricted query language expressiveness in the context of the advanced capabilities demonstrated by contemporary MLLMs.7 This provides a more accurate assessment of the system's relative standing.  
* **Suggest MLLM-Inspired Future Directions:**  
  * *Hybrid Systems:* Propose investigating hybrid architectures as a future direction. This could involve using the current system for efficient initial candidate generation based on geometric constraints, followed by MLLM-based re-ranking to incorporate semantic understanding and handle more complex spatial queries.  
  * *Semantic Integration:* Suggest exploring methods to augment the n-gram matching with semantic similarity measures (e.g., using word or phrase embeddings) to improve robustness to variations in wording, bridging the gap towards MLLM capabilities.  
* **Incorporate Advanced Evaluation:** Recommend exploring grounding-specific evaluation metrics, drawing inspiration from frameworks like SMuDGE 10, as part of future work. This is particularly relevant when evaluating on real-world data, as it would allow for a more direct assessment of spatial localization accuracy beyond simple image retrieval metrics.  
* **Address Scalability:** Reiterate the scalability concern related to the in-memory index for very large datasets. Suggest investigating disk-based, distributed, or approximate indexing techniques (potentially referencing Learned Sparse Retrieval methods mentioned in the original draft's prior work 1 or other efficient retrieval paradigms like hashing 17) as necessary future work for large-scale deployment.  
* **Explore Advanced Spatial Queries:** Propose extending the system to support more complex spatial queries beyond single rectangles (e.g., relative positioning, non-rectangular shapes) as a future research avenue, potentially drawing inspiration from the capabilities of MLLMs or other spatial reasoning frameworks like scene graphs.18

By incorporating these updates, the paper can more accurately position its contributions within the current state-of-the-art, acknowledge its limitations realistically, and outline more informed and relevant directions for future research.

#### **Works cited**

1. FoundationVision/Groma: \[ECCV2024\] Grounded Multimodal Large Language Model with Localized Visual Tokenization \- GitHub, accessed April 23, 2025, [https://github.com/FoundationVision/Groma](https://github.com/FoundationVision/Groma)  
2. A Concept-Based Explainability Framework for Large Multimodal Models \- OpenReview, accessed April 23, 2025, [https://openreview.net/forum?id=MvjLRFntW6\&referrer=%5Bthe%20profile%20of%20Matthieu%20Cord%5D(%2Fprofile%3Fid%3D\~Matthieu\_Cord1)](https://openreview.net/forum?id=MvjLRFntW6&referrer=%5Bthe+profile+of+Matthieu+Cord%5D\(/profile?id%3D~Matthieu_Cord1\))  
3. ESA: External Space Attention Aggregation for Image-Text Retrieval | Request PDF, accessed April 23, 2025, [https://www.researchgate.net/publication/369045493\_ESA\_External\_Space\_Attention\_Aggregation\_for\_Image-Text\_Retrieval](https://www.researchgate.net/publication/369045493_ESA_External_Space_Attention_Aggregation_for_Image-Text_Retrieval)  
4. \[PDF\] UniIR: Training and Benchmarking Universal Multimodal Information Retrievers, accessed April 23, 2025, [https://www.semanticscholar.org/paper/9037c23cd2b25eea8f146e73ba6ad7985549d1a6](https://www.semanticscholar.org/paper/9037c23cd2b25eea8f146e73ba6ad7985549d1a6)  
5. NeurIPS Poster A Concept-Based Explainability Framework for Large Multimodal Models, accessed April 23, 2025, [https://neurips.cc/virtual/2024/poster/95482](https://neurips.cc/virtual/2024/poster/95482)  
6. Semi-Supervised Multimodal Representation Learning Through a Global Workspace, accessed April 23, 2025, [https://www.researchgate.net/publication/381923179\_Semi-Supervised\_Multimodal\_Representation\_Learning\_Through\_a\_Global\_Workspace](https://www.researchgate.net/publication/381923179_Semi-Supervised_Multimodal_Representation_Learning_Through_a_Global_Workspace)  
7. GROUNDING MULTIMODAL LARGE LANGUAGE MODELS TO THE WORLD \- OpenReview, accessed April 23, 2025, [https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf](https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf)  
8. \[2306.14824\] Kosmos-2: Grounding Multimodal Large Language Models to the World \- ar5iv, accessed April 23, 2025, [https://ar5iv.labs.arxiv.org/html/2306.14824](https://ar5iv.labs.arxiv.org/html/2306.14824)  
9. GPT-4 enhanced multimodal grounding for autonomous driving: Leveraging cross-modal attention with large language models \- SciOpen, accessed April 23, 2025, [https://www.sciopen.com/article/10.1016/j.commtr.2023.100116](https://www.sciopen.com/article/10.1016/j.commtr.2023.100116)  
10. Where is this coming from? Making groundedness count in the evaluation of Document VQA models \- arXiv, accessed April 23, 2025, [https://arxiv.org/html/2503.19120v1](https://arxiv.org/html/2503.19120v1)  
11. Where is this coming from? Making groundedness count in the evaluation of Document VQA models \- ResearchGate, accessed April 23, 2025, [https://www.researchgate.net/publication/390177225\_Where\_is\_this\_coming\_from\_Making\_groundedness\_count\_in\_the\_evaluation\_of\_Document\_VQA\_models/download](https://www.researchgate.net/publication/390177225_Where_is_this_coming_from_Making_groundedness_count_in_the_evaluation_of_Document_VQA_models/download)  
12. Towards Multi-Modal Co-Reference Resolution in Conversational Shopping Agents \- ACL Anthology, accessed April 23, 2025, [https://aclanthology.org/2024.ecnlp-1.2.pdf](https://aclanthology.org/2024.ecnlp-1.2.pdf)  
13. UCLA Electronic Theses and Dissertations \- eScholarship.org, accessed April 23, 2025, [https://escholarship.org/content/qt5rh3w127/qt5rh3w127.pdf?t=sejyhw](https://escholarship.org/content/qt5rh3w127/qt5rh3w127.pdf?t=sejyhw)  
14. Applications of Large Language Models and Multimodal Large Models in Autonomous Driving: A Comprehensive Review \- MDPI, accessed April 23, 2025, [https://www.mdpi.com/2504-446X/9/4/238](https://www.mdpi.com/2504-446X/9/4/238)  
15. NeurIPS Poster LOVA3: Learning to Visual Question Answering, Asking and Assessment, accessed April 23, 2025, [https://neurips.cc/virtual/2024/poster/93210](https://neurips.cc/virtual/2024/poster/93210)  
16. Elias Stengel-Eskin, accessed April 23, 2025, [https://esteng.github.io/](https://esteng.github.io/)  
17. Efficient Query-based Black-box Attack against Cross-modal Hashing Retrieval, accessed April 23, 2025, [https://www.semanticscholar.org/paper/a471ef4fae2c4748eac0c13c4b434e7f701c3252](https://www.semanticscholar.org/paper/a471ef4fae2c4748eac0c13c4b434e7f701c3252)  
18. 52CV/WACV-2025-Papers \- GitHub, accessed April 23, 2025, [https://github.com/52CV/WACV-2025-Papers](https://github.com/52CV/WACV-2025-Papers)