**Plan for Synthetic Data and Query Generation (Consistent Detail + End Results)**

1.  **Setup & Dependencies:**

    - **Plan:** Identify necessary Python libraries: `Pillow` (for image creation and drawing), `Faker` (for generating random text), `random`. Define configuration parameters: Image dimensions (Width `W=640`, Height `H=360`), number of images (`NUM_IMAGES`, e.g., 50), number of queries per image (`NUM_QUERIES_PER_IMAGE`, e.g., 10), output directories (`./synthetic_data/images/`, `./synthetic_data/metadata/`), font settings (path to a standard TTF font, trying default first; font size, e.g., 16), word spacing (pixels, e.g., 5), line spacing factor (e.g., 1.4), margins (pixels, e.g., 10), N-gram range (min=1, max=3).
    - **End Result:** The development environment is confirmed to have the necessary libraries (`Pillow`, `Faker`). All configuration constants (`W`, `H`, `NUM_IMAGES`, `NUM_QUERIES_PER_IMAGE`, paths, font details, spacing, margins, N-gram range) are defined and accessible (e.g., as variables in a script). We have a clear specification of the data to be generated (quantity, format, location).

2.  **Sentence Pool Generation:**

    - **Plan:** Create a fixed pool of unique sentences (`SENTENCE_POOL`) using `Faker` (e.g., generate 100-200 sentences). This pool will be the source text for all images, ensuring controlled word/phrase repetition across the dataset for testing spatial differentiation.
    - **End Result:** A Python list variable (`SENTENCE_POOL`) exists, holding the generated unique sentences. This list is ready to be sampled from during image generation, providing the mechanism for text repetition across images.

3.  **Image Generation Module (`generate_image_and_metadata`)**

    - **Plan:** Implement a Python function `generate_image_and_metadata`. This function will take `image_id`, `SENTENCE_POOL`, output directory, dimensions, font settings, spacing, and margins as input. Inside, it will: create a blank white image using `Pillow`; initialize `ImageDraw` and load the font; randomly sample sentences from `SENTENCE_POOL`, join, and split into words; iterate through words, drawing them from top-left, handling line wrapping (if word exceeds right margin) and stopping if text exceeds bottom margin; use `draw.textbbox` _before_ drawing to get precise placement; record the exact pixel bounding box `[top, left, bottom, right]` for each successfully drawn word along with its text; save the final image as a `.png` file (e.g., `synth_001.png`). The function will return the saved image file path and a dictionary containing `image_id`, `width`, `height`, and the list of `word_data` (`{'text': str, 'bbox_pixels': [t, l, b, r]}`).
    - **End Result:** We possess a _callable Python function_ `generate_image_and_metadata` that performs the image creation and word-level annotation. Executing this function produces one correctly formatted PNG image file and returns the crucial metadata linking each word to its ground truth pixel coordinates on that image.

4.  **N-gram Calculation Module (`calculate_ngrams`)**

    - **Plan:** Implement a Python function `calculate_ngrams`. This function takes the `word_data` list (output from step 3) as input. It will iterate through n-gram lengths `n` from 1 up to 3. For each `n`, it iterates through the `word_data` to form all possible consecutive sequences of `n` words. For each sequence, it concatenates the word texts (space-separated) and calculates the precise union bounding box in pixels (`[min_top, min_left, max_bottom, max_right]`) based on the bounding boxes of the constituent words. It returns a list of `ngram_data` dictionaries, each containing `{'text': str, 'bbox_pixels': [t, l, b, r], 'words': [list_of_n_word_dicts]}`.
    - **End Result:** We possess a _callable Python function_ `calculate_ngrams`. When provided with the word data from an image, this function accurately computes all 1-, 2-, and 3-grams present and determines their exact combined pixel bounding boxes, preparing the data needed for query generation.

5.  **Query Generation Module (`generate_queries_for_image`)**

    - **Plan:** Implement a Python function `generate_queries_for_image`. Inputs: `image_id`, `image_width`, `image_height`, the `ngram_data` list (output from step 4), and `num_queries`. Inside, it will loop `num_queries` times. In each iteration: randomly select a target n-gram from `ngram_data`; normalize the target n-gram's pixel bbox to percentage coordinates `target_bbox_norm = [t/H*100, l/W*100, b/H*100, r/W*100]`; randomly choose a query type according to the specified distribution (`No Region` 20%, `Exact Match` 20%, `High IoU` 20%, `Low IoU` 20%, `Nearby` 10%, `Distant` 10%); generate the corresponding `query_region_norm` (`[q_top, q_left, q_bottom, q_right]` percentages) based on the chosen type and the `target_bbox_norm` (using `[0.0, 0.0, 100.0, 100.0]` for 'No Region', exact copy for 'Exact Match', and calculated offsets/sizes for others, ensuring bounds [0-100]); create and store a query dictionary containing `query_id`, `image_id`, `query_text` (from target), `query_region_norm`, `target_ngram_bbox_pixels` (ground truth), `target_ngram_bbox_norm`, and `generation_type`. The function returns the list of these generated query dictionaries.
    - **End Result:** We possess a _callable Python function_ `generate_queries_for_image`. Given the n-grams for an image, it generates the specified number of diverse test queries. Each query simulates a potential user search (text + optional region) and includes metadata about how it was generated (e.g., intended IoU overlap) and the ground truth location of the text it targets, facilitating later evaluation.

6.  **Main Orchestration Script:**

    - **Plan:** Write the main Python script that ties everything together. This script will: define or import the configuration constants (Step 1); create the output directories (`./synthetic_data/images/`, `./synthetic_data/metadata/`) if they don't exist; call the function to generate the `SENTENCE_POOL` (Step 2); initialize empty lists `all_image_metadata = []` and `all_queries = []`; loop `NUM_IMAGES` times: generate an `image_id`, call `generate_image_and_metadata` (Step 3), call `calculate_ngrams` (Step 4) using the result, store the combined image metadata (including n-grams) in `all_image_metadata`, call `generate_queries_for_image` (Step 5) using the results, and extend `all_queries` with the generated queries; finally, after the loop, save the contents of `all_image_metadata` to `synthetic_data/metadata/image_metadata.json` and `all_queries` to `synthetic_data/metadata/queries.json` using the `json` library.
    - **End Result:** We have a _single, executable Python script_. Running this script initiates and completes the entire data generation workflow, calling the functions defined in steps 2-5 in the correct sequence and handling the data aggregation and file output.

7.  **Output Files:**
    - **Plan:** The main orchestration script (Step 6) runs to completion without errors.
    - **End Result:** The generation process concludes successfully, leaving the following artifacts on the filesystem:
      - The directory `./synthetic_data/images/` populated with `NUM_IMAGES` (e.g., 50) `.png` image files, each containing randomly selected text from the pool laid out according to the rules.
      - The file `./synthetic_data/metadata/image_metadata.json` containing a single JSON list. Each element in the list is a dictionary describing one image (`image_id`, dimensions) and includes the detailed `word_data` and `ngram_data` (text and pixel bounding boxes) for that image.
      - The file `./synthetic_data/metadata/queries.json` containing a single JSON list of all generated queries (`NUM_IMAGES * NUM_QUERIES_PER_IMAGE`, e.g., 500). Each query dictionary specifies the target `image_id`, `query_text`, `query_region_norm`, ground truth pixel/normalized locations of the text it was based on, and the `generation_type`. This complete dataset is now ready for use in developing and testing the subsequent indexing and search phases.

This breakdown maintains the detailed plan for each step while explicitly stating the concrete outcome or capability achieved upon that step's completion. Let me know when you're ready for me to start implementing this.
